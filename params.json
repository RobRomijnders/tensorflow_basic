{
  "name": "Start on TensorBoard",
  "tagline": "Start of TensorFlow with TensorBoard",
  "body": "### Start on TensorBoard\r\nWith the increasing popularity of TensorFlow, I also explored its use this week. Both Google as Youtubers made nice tutorials to get used to the world of placeholders and sessions.\r\n\r\nIn making my first project work on the MNIST, I browsed the forums for answers to my questions. Many people asked for some example code to work from, rather than tutorials. Therefore, I decided to share my code in this preliminary stage.\r\n\r\nFor the quick forum-browsers, here is a working code. The updated version is on my GitHub in the repository tensorflow_basic\r\n\r\n```python\r\n# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Tue Mar 22 10:43:29 2016\r\n\r\n@author: rob\r\n\"\"\"\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport input_data\r\nfrom numpy import genfromtxt\r\nsess = tf.InteractiveSession()\r\n\r\n\r\n#%For quick debugging, we use a two-class version of the MNIST,\r\n#where the targets are encoded one-hot.\r\n# You can use any variation of MNIST. As long as you make sure\r\n#that y_train and y_test are one-hot and X_train and X_test have\r\n#the samples ordered in rows\r\nX_test = genfromtxt('X_test.csv', delimiter=',')\r\ny_test = genfromtxt('y_test.csv', delimiter=',')\r\nX_train = genfromtxt('X_train.csv', delimiter=',')\r\ny_train = genfromtxt('y_train.csv', delimiter=',')\r\n\r\nN = X_train.shape[0]\r\nNtest = X_test.shape[0]\r\n\r\n#Check for the input sizes\r\nassert (N>X_train.shape[1]), 'You are feeding a fat matrix for training, are you sure?'\r\nassert (Ntest>X_test.shape[1]), 'You are feeding a fat matrix for testing, are you sure?'\r\nassert (y_train.shape[0]>y_train.shape[1]), 'You are feedinf a fat matrix for labels, are you sure?'\r\n\r\n# Nodes for the input variables\r\nx = tf.placeholder(\"float\", shape=[None, 784], name = 'Input_data')\r\ny_ = tf.placeholder(\"float\", shape=[None, 10], name = 'Ground_truth')\r\n\r\n\r\n# Define functions for initializing variables and standard layers\r\n#For now, this seems superfluous, but in extending the code\r\n#to many more layers, this will keep our code\r\n#read-able\r\ndef weight_variable(shape, name):\r\n  initial = tf.truncated_normal(shape, stddev=0.1)\r\n  return tf.Variable(initial, name = name)\r\n\r\ndef bias_variable(shape, name):\r\n  initial = tf.constant(0.1, shape=shape)\r\n  return tf.Variable(initial, name = name)\r\n\r\ndef conv2d(x, W):\r\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\r\n\r\ndef max_pool_2x2(x):\r\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\r\n                        strides=[1, 2, 2, 1], padding='SAME')\r\n\r\nwith tf.name_scope(\"Reshaping_data\") as scope:\r\n  x_image = tf.reshape(x, [-1,28,28,1])\r\n  image_summ = tf.image_summary(\"Example_images\", x_image)\r\n\r\nwith tf.name_scope(\"Conv1\") as scope:\r\n  W_conv1 = weight_variable([5, 5, 1, 32], 'Conv_Layer_1')\r\n  b_conv1 = bias_variable([32], 'bias_for_Conv_Layer_1')\r\n  h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\r\n  h_pool1 = max_pool_2x2(h_conv1)\r\n  \r\nwith tf.name_scope('Visualize_filters') as scope:\r\n    \r\n# In this section, we visualize the filters of the first convolutional layers\r\n# We concatenate the filters into one image\r\n# Credits for the inspiration go to Martin Gorner\r\n  W1_a = W_conv1                       # [5, 5, 1, 32]\r\n  W1pad= tf.zeros([5, 5, 1, 1])        # [5, 5, 1, 4]  - four zero kernels for padding\r\n  # We have a 6 by 6 grid of kernepl visualizations. yet we only have 32 filters\r\n  # Therefore, we concatenate 4 empty filters\r\n  W1_b = tf.concat(3, [W1_a, W1pad, W1pad, W1pad, W1pad])   # [5, 5, 1, 36]  \r\n  W1_c = tf.split(3, 36, W1_b)         # 36 x [5, 5, 1, 1]\r\n  W1_row0 = tf.concat(0, W1_c[0:6])    # [30, 5, 1, 1]\r\n  W1_row1 = tf.concat(0, W1_c[6:12])   # [30, 5, 1, 1]\r\n  W1_row2 = tf.concat(0, W1_c[12:18])  # [30, 5, 1, 1]\r\n  W1_row3 = tf.concat(0, W1_c[18:24])  # [30, 5, 1, 1]\r\n  W1_row4 = tf.concat(0, W1_c[24:30])  # [30, 5, 1, 1]\r\n  W1_row5 = tf.concat(0, W1_c[30:36])  # [30, 5, 1, 1]\r\n  W1_d = tf.concat(1, [W1_row0, W1_row1, W1_row2, W1_row3, W1_row4, W1_row5]) # [30, 30, 1, 1]\r\n  W1_e = tf.reshape(W1_d, [1, 30, 30, 1])\r\n  Wtag = tf.placeholder(tf.string, None)\r\n  tf.image_summary(\"Visualize_kernels\", W1_e)\r\n\r\n# The name_scope lines serve to organize our graphs that TensorFlow will create\r\n# for us\r\nwith tf.name_scope(\"Conv2\") as scope:\r\n  W_conv2 = weight_variable([5, 5, 32, 64], 'Conv_Layer_2')\r\n  b_conv2 = bias_variable([64], 'bias_for_Conv_Layer_2')\r\n  h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\r\n  h_pool2 = max_pool_2x2(h_conv2)\r\n\r\nwith tf.name_scope(\"Fully_Connected1\") as scope:\r\n  W_fc1 = weight_variable([7 * 7 * 64, 1024], 'Fully_Connected_layer_1')\r\n  b_fc1 = bias_variable([1024], 'bias_for_Fully_Connected_Layer_1')\r\n  h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\r\n  h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\r\n\r\nwith tf.name_scope(\"Fully_Connected2\") as scope:\r\n  keep_prob = tf.placeholder(\"float\")\r\n  h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\r\n  \r\n  W_fc2 = tf.Variable(tf.truncated_normal([1024, 10], stddev=0.1),name = 'W_fc2')\r\n  b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]),name = 'b_fc2')\r\n\r\nwith tf.name_scope(\"Final_Softmax\") as scope:\r\n  y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\r\n\r\n# Also add histograms to TensorBoard\r\nw_hist = tf.histogram_summary(\"W_fc2\", W_fc2)\r\nb_hist = tf.histogram_summary(\"b_fc2\", b_fc2)\r\n\r\nwith tf.name_scope(\"Entropy\") as scope:\r\n    cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\r\n    ce_summ = tf.scalar_summary(\"cross entropy\", cross_entropy)\r\nwith tf.name_scope(\"train\") as scope:\r\n    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\r\nwith tf.name_scope(\"Evaluating\") as scope:\r\n    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\r\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\r\n    accuracy_summary = tf.scalar_summary(\"accuracy\", accuracy)\r\n    \r\n    \r\nmerged = tf.merge_all_summaries()\r\nwriter = tf.train.SummaryWriter(\"/home/rob/Dropbox/ConvNets/tf/log_tb\", sess.graph_def)\r\n\r\nsess.run(tf.initialize_all_variables())\r\nfor i in range(500):\r\n  batch_ind = np.random.choice(N,50,replace=False)\r\n  if i%100 == 0:\r\n    result = sess.run([accuracy,merged], feed_dict={ x: X_test, y_: y_test, keep_prob: 1.0})\r\n    acc = result[0]\r\n    summary_str = result[1]\r\n    writer.add_summary(summary_str, i)\r\n    writer.flush()  #Don't forget this command! It makes sure Python writes the summaries to the log-file\r\n    print(\"Accuracy at step %s: %s\" % (i, acc))\r\n\r\n  sess.run(train_step,feed_dict={x:X_train[batch_ind], y_: y_train[batch_ind], keep_prob: 0.5})\r\n \r\nsess.close()\r\n\r\n# We can now open TensorBoard. Run the following line from your terminal\r\n# tensorboard --logdir=/home/rob/Dropbox/ConvNets/tf/log_tb\r\n```\r\nAs always, I am curious to any comments and questions. Reach me at romijndersrob@gmail.com",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}